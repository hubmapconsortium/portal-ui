{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from portal_visualization.builder_factory import get_view_config_builder\n",
    "from portal_visualization.constants import image_units\n",
    "from portal_visualization.utils import get_image_scale, get_physical_size_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your authentication token, pipeline value, and environment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication token (Globus groups token)\n",
    "# easiest way to retrieve = log in to environment, open dev tools, and copy the value of `Authentication` header from any request to the search API\n",
    "TOKEN = \"\"\n",
    "\n",
    "if not TOKEN:\n",
    "    import warnings;\n",
    "    warnings.warn(\"Authentication token is required to view QA datasets. Results will only include public datasets. Please set the TOKEN variable to view private datasets.\")\n",
    "\n",
    "# Pipeline value to search for\n",
    "PIPELINE = \"Kaggle-1 Glomerulus Segmentation\"\n",
    "\n",
    "if not PIPELINE:\n",
    "    raise ValueError(\"Pipeline value is required. Please set the PIPELINE variable.\")\n",
    "\n",
    "# Environment: \"dev,\" \"test,\" or \"prod\"\n",
    "ENV = \"prod\"\n",
    "\n",
    "\n",
    "if ENV not in [\"dev\", \"test\", \"prod\"]:\n",
    "    raise ValueError(f\"Invalid environment: {ENV}. Must be 'dev', 'test', or 'prod'.\")\n",
    "\n",
    "ENDPOINTS = {\n",
    "    \"dev\": {\n",
    "        \"search_api\": \"https://search-api.dev.hubmapconsortium.org/v3/portal/search\",\n",
    "        \"assets\": \"https://assets.dev.hubmapconsortium.org\",\n",
    "    },\n",
    "    \"prod\": {\n",
    "        \"search_api\": \"https://search.api.hubmapconsortium.org/v3/portal/search\",\n",
    "        \"assets\": \"https://assets.hubmapconsortium.org\",\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"search_api\": \"https://search-api.test.hubmapconsortium.org/v3/portal/search\",\n",
    "        \"assets\": \"https://assets.test.hubmapconsortium.org\",\n",
    "    }\n",
    "}\n",
    "\n",
    "SEARCH_API = ENDPOINTS[ENV][\"search_api\"]\n",
    "ASSETS_URL = ENDPOINTS[ENV][\"assets\"]\n",
    "\n",
    "print(f\"Looking for {PIPELINE} datasets in {ENV} environment.{' (including private datasets)' if TOKEN else ' (public datasets only)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers():\n",
    "    headers = {}\n",
    "    if TOKEN:\n",
    "        headers[\"Authorization\"] = f\"Bearer {TOKEN}\"\n",
    "    return headers\n",
    "\n",
    "\n",
    "def search_api_query(query):\n",
    "    \"\"\"Execute a search API query and return hits.\"\"\"\n",
    "    response = requests.post(SEARCH_API, headers=get_headers(), json=query)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return data[\"hits\"][\"hits\"]\n",
    "\n",
    "\n",
    "def get_entity_by_uuid(uuid):\n",
    "    \"\"\"Fetch a single entity from the search API.\"\"\"\n",
    "    query = {\"query\": {\"ids\": {\"values\": [uuid]}}}\n",
    "    hits = search_api_query(query)\n",
    "    if hits:\n",
    "        return hits[0][\"_source\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_support_entity(parent_uuid):\n",
    "    \"\"\"Find the most recent is_support + is_image descendant of the given UUID.\n",
    "\n",
    "    Mirrors client.py's get_descendant_to_lift() method.\n",
    "    Returns the most recently modified support entity, or None.\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"term\": {\"vitessce-hints\": \"is_support\"}},\n",
    "                    {\"term\": {\"vitessce-hints\": \"is_image\"}},\n",
    "                    {\"term\": {\"ancestor_ids\": parent_uuid}},\n",
    "                    {\"terms\": {\"mapped_status.keyword\": [\"QA\", \"Published\"]}},\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"sort\": [{\"last_modified_timestamp\": {\"order\": \"desc\"}}],\n",
    "        \"size\": 1,\n",
    "    }\n",
    "    hits = search_api_query(query)\n",
    "    if hits:\n",
    "        return hits[0][\"_source\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "def vis_lift_entity(entity, max_depth=3):\n",
    "    \"\"\"Replicate the portal's vis-lifting chain (client.py:get_vitessce_conf_cells_and_lifted_uuid).\n",
    "\n",
    "    The portal doesn't directly build configs from the entity you're viewing.\n",
    "    Instead, it checks if the entity has a newer support descendant (is_support + is_image)\n",
    "    and recursively follows that chain. The final entity is what gets visualized,\n",
    "    with the previous entity in the chain as its parent.\n",
    "\n",
    "    Returns (entity_to_visualize, parent_entity, vis_lifted_uuid or None).\n",
    "    \"\"\"\n",
    "    descendant = find_support_entity(entity[\"uuid\"])\n",
    "\n",
    "    if descendant and max_depth > 0:\n",
    "        # Ensure files are available on the descendant\n",
    "        if not descendant.get(\"files\") and descendant.get(\"metadata\", {}).get(\"files\"):\n",
    "            descendant[\"files\"] = descendant[\"metadata\"][\"files\"]\n",
    "\n",
    "        if descendant.get(\"files\") or descendant.get(\"metadata\", {}).get(\"files\"):\n",
    "            # Vis-lift: the descendant becomes the entity to visualize,\n",
    "            # the current entity becomes the parent\n",
    "            vis_lifted_uuid = descendant[\"uuid\"]\n",
    "\n",
    "            # Recursively check if the descendant also has a support descendant\n",
    "            inner_entity, inner_parent, inner_lifted = vis_lift_entity(\n",
    "                descendant, max_depth=max_depth - 1\n",
    "            )\n",
    "\n",
    "            # If inner vis-lifting happened, use that result\n",
    "            if inner_lifted:\n",
    "                return inner_entity, inner_parent, inner_lifted\n",
    "\n",
    "            # Otherwise, the descendant is the entity, current is the parent\n",
    "            return descendant, entity, vis_lifted_uuid\n",
    "\n",
    "    # No vis-lifting: return entity with no parent (caller will resolve parent)\n",
    "    return entity, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch All Datasets with Input Pipeline Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"term\": {\"entity_type.keyword\": \"Dataset\"}},\n",
    "                {\"term\": {\"pipeline.keyword\": PIPELINE}},\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"size\": 10000,\n",
    "    \"_source\": [\n",
    "        \"uuid\",\n",
    "        \"hubmap_id\",\n",
    "        \"pipeline\",\n",
    "        \"immediate_ancestor_ids\",\n",
    "        \"files\",\n",
    "        \"vitessce-hints\",\n",
    "        \"soft_assaytype\",\n",
    "        \"mapped_status\",\n",
    "        \"status\",\n",
    "        \"metadata\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "pipeline_hits = search_api_query(pipeline_query)\n",
    "datasets = [hit[\"_source\"] for hit in pipeline_hits]\n",
    "print(f\"Found {len(datasets)} datasets with pipeline '{PIPELINE}'\")\n",
    "\n",
    "for ds in datasets:\n",
    "    parent_ids = ds.get(\"immediate_ancestor_ids\", [])\n",
    "    print(\n",
    "        f\"  {ds.get('hubmap_id', 'N/A')} ({ds['uuid'][:8]}...) \"\n",
    "        f\"status={ds.get('mapped_status', ds.get('status', 'unknown'))} \"\n",
    "        f\"parents={parent_ids}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Vitessce Configurations\n",
    "\n",
    "For each dataset, replicate the portal's vis-lifting chain: check if the entity\n",
    "has a newer support descendant, and if so, use that descendant for config generation.\n",
    "This matches how the portal actually generates configurations at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for ds in datasets:\n",
    "    uuid = ds[\"uuid\"]\n",
    "    hubmap_id = ds.get(\"hubmap_id\", \"N/A\")\n",
    "    parent_ids = ds.get(\"immediate_ancestor_ids\", [])\n",
    "\n",
    "    print(\"Processing dataset {} ({}) with parents {}\".format(hubmap_id, uuid[:8], parent_ids))\n",
    "\n",
    "    # Ensure files are present in the expected format\n",
    "    if not ds.get(\"files\") and ds.get(\"metadata\", {}).get(\"files\"):\n",
    "        ds[\"files\"] = ds[\"metadata\"][\"files\"]\n",
    "\n",
    "    if not ds.get(\"files\"):\n",
    "        results.append(\n",
    "            {\n",
    "                \"uuid\": uuid,\n",
    "                \"hubmap_id\": hubmap_id,\n",
    "                \"parent_uuid\": parent_ids[0] if parent_ids else None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"No files found in entity\",\n",
    "                \"base_image_source\": None,\n",
    "                \"conf\": None,\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    parent_uuid = parent_ids[0] if parent_ids else None\n",
    "    if parent_uuid is None:\n",
    "        results.append(\n",
    "            {\n",
    "                \"uuid\": uuid,\n",
    "                \"hubmap_id\": hubmap_id,\n",
    "                \"parent_uuid\": None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"No parent UUID found\",\n",
    "                \"base_image_source\": None,\n",
    "                \"conf\": None,\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Replicate the portal's vis-lifting chain:\n",
    "        # Check if this entity has a newer support descendant, and if so,\n",
    "        # use that descendant as the entity to build the config for.\n",
    "        entity_to_viz, vis_parent, vis_lifted_uuid = vis_lift_entity(ds)\n",
    "\n",
    "        # If vis-lifting happened, use the vis-lifted entity/parent.\n",
    "        # Otherwise, resolve the parent from the original entity's ancestors.\n",
    "        if vis_lifted_uuid:\n",
    "            print(\n",
    "                f\"  {hubmap_id}: vis-lifted to {vis_lifted_uuid[:8]}... \"\n",
    "                f\"(parent={vis_parent['uuid'][:8]}...)\"\n",
    "            )\n",
    "        else:\n",
    "            parent_entity = get_entity_by_uuid(parent_uuid)\n",
    "            entity_to_viz = ds\n",
    "            vis_parent = parent_entity or {\"uuid\": parent_uuid}\n",
    "\n",
    "        # get_view_config_builder expects a UUID string for parent, not a dict\n",
    "        parent_uuid_str = vis_parent[\"uuid\"] if isinstance(vis_parent, dict) else vis_parent\n",
    "\n",
    "        Builder = get_view_config_builder(\n",
    "            entity_to_viz, get_entity_by_uuid, parent_uuid_str\n",
    "        )\n",
    "        builder = Builder(\n",
    "            entity_to_viz,\n",
    "            TOKEN,\n",
    "            ASSETS_URL,\n",
    "            get_entity=get_entity_by_uuid,\n",
    "            parent=vis_parent,\n",
    "            find_support_entity=find_support_entity,\n",
    "        )\n",
    "\n",
    "        conf_cells = builder.get_conf_cells()\n",
    "\n",
    "        # Read the debug flag\n",
    "        base_image_source = getattr(builder, \"base_image_source\", \"unknown\")\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"uuid\": uuid,\n",
    "                \"hubmap_id\": hubmap_id,\n",
    "                \"parent_uuid\": parent_uuid,\n",
    "                \"vis_lifted_uuid\": vis_lifted_uuid,\n",
    "                \"entity_visualized\": entity_to_viz[\"uuid\"],\n",
    "                \"status\": \"success\",\n",
    "                \"error\": None,\n",
    "                \"base_image_source\": base_image_source,\n",
    "                \"conf\": conf_cells.conf,\n",
    "                \"builder_class\": builder.__class__.__name__,\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"  OK: {hubmap_id} - source={base_image_source}, \"\n",
    "            f\"builder={builder.__class__.__name__}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        results.append(\n",
    "            {\n",
    "                \"uuid\": uuid,\n",
    "                \"hubmap_id\": hubmap_id,\n",
    "                \"parent_uuid\": parent_uuid,\n",
    "                \"vis_lifted_uuid\": None,\n",
    "                \"entity_visualized\": None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"base_image_source\": None,\n",
    "                \"conf\": None,\n",
    "            }\n",
    "        )\n",
    "        print(f\"  FAIL: {hubmap_id} - {e}\")\n",
    "\n",
    "succeeded = sum(1 for r in results if r[\"status\"] == \"success\")\n",
    "failed = sum(1 for r in results if r[\"status\"] == \"error\")\n",
    "vis_lifted = sum(1 for r in results if r.get(\"vis_lifted_uuid\"))\n",
    "print(f\"\\nSucceeded: {succeeded}, Failed: {failed}, Vis-lifted: {vis_lifted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract OME-TIFF URLs from Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_conf(conf):\n",
    "    \"\"\"Extract base image and segmentation image URLs from a Vitessce config.\"\"\"\n",
    "    info = {\n",
    "        \"base_image_url\": None,\n",
    "        \"seg_image_url\": None,\n",
    "        \"coordinate_transformations\": None,\n",
    "    }\n",
    "\n",
    "    if conf is None:\n",
    "        return info\n",
    "\n",
    "    datasets = conf.get(\"datasets\", [])\n",
    "    if not datasets:\n",
    "        return info\n",
    "\n",
    "    files = datasets[0].get(\"files\", [])\n",
    "    for f in files:\n",
    "        file_type = f.get(\"fileType\", \"\")\n",
    "        if file_type == \"image.ome-tiff\":\n",
    "            info[\"base_image_url\"] = f.get(\"url\")\n",
    "        elif file_type == \"obsSegmentations.ome-tiff\":\n",
    "            info[\"seg_image_url\"] = f.get(\"url\")\n",
    "            transforms = f.get(\"options\", {}).get(\n",
    "                \"coordinateTransformations\", []\n",
    "            )\n",
    "            if transforms:\n",
    "                info[\"coordinate_transformations\"] = transforms[0].get(\"scale\")\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "for r in results:\n",
    "    if r[\"status\"] == \"success\":\n",
    "        urls = extract_urls_from_conf(r[\"conf\"])\n",
    "        r.update(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fetch OME-TIFF Metadata\n",
    "\n",
    "For each image URL in the generated configs, fetch metadata containing pixel dimensions\n",
    "and physical sizes. First tries the `metadata.json` sidecar files; for older datasets\n",
    "where these don't exist, falls back to reading the OME XML directly from the TIFF header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n",
    "\n",
    "\n",
    "def metadata_url_from_image_url(image_url):\n",
    "    \"\"\"Convert an OME-TIFF image URL to its corresponding metadata.json URL.\n",
    "\n",
    "    Pattern: ometiff-pyramids/X.ome.tif -> image_metadata/X.metadata.json\n",
    "    \"\"\"\n",
    "    if image_url is None:\n",
    "        return None\n",
    "    url = re.sub(r\"ometiff-pyramids\", \"image_metadata\", image_url)\n",
    "    url = re.sub(r\"\\.ome\\.tiff?\", \".metadata.json\", url)\n",
    "    return url\n",
    "\n",
    "\n",
    "def fetch_metadata_json(url):\n",
    "    \"\"\"Fetch a metadata.json sidecar file from the assets endpoint.\n",
    "\n",
    "    Returns the parsed JSON dict on success, or None silently on 404\n",
    "    (older datasets don't have these files).\n",
    "    \"\"\"\n",
    "    if url is None:\n",
    "        return None\n",
    "    try:\n",
    "        resp = requests.get(url, headers=get_headers(), timeout=30)\n",
    "        if resp.status_code == 200:\n",
    "            data = resp.json()\n",
    "            if isinstance(data, dict):\n",
    "                return data\n",
    "        # 404 is expected for older datasets — fall through silently\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_ome_metadata_from_tiff(image_url):\n",
    "    \"\"\"Read OME metadata directly from a remote OME-TIFF file header.\n",
    "\n",
    "    Uses tifffile with fsspec's HTTPFileSystem to make HTTP range requests,\n",
    "    reading only the TIFF IFDs and OME XML without downloading the full pyramid.\n",
    "\n",
    "    Returns a dict with SizeX, SizeY, PhysicalSizeX, PhysicalSizeY,\n",
    "    PhysicalSizeXUnit, PhysicalSizeYUnit (matching metadata.json format),\n",
    "    or None on failure.\n",
    "    \"\"\"\n",
    "    if image_url is None:\n",
    "        return None\n",
    "    try:\n",
    "        import fsspec\n",
    "        import tifffile\n",
    "\n",
    "        # Parse the URL to extract query params (e.g. ?token=...) as headers\n",
    "        parsed = urlparse(image_url)\n",
    "        params = parse_qs(parsed.query)\n",
    "        # Rebuild URL without query string for fsspec\n",
    "        clean_url = urlunparse(parsed._replace(query=\"\"))\n",
    "\n",
    "        # Pass token as Authorization header if present in query params\n",
    "        storage_options = {}\n",
    "        if \"token\" in params:\n",
    "            storage_options[\"headers\"] = {\n",
    "                \"Authorization\": f\"Bearer {params['token'][0]}\"\n",
    "            }\n",
    "\n",
    "        with fsspec.open(clean_url, mode=\"rb\", **storage_options) as f:\n",
    "            with tifffile.TiffFile(f) as tif:\n",
    "                result = {}\n",
    "\n",
    "                # Get pixel dimensions from first page\n",
    "                if tif.pages:\n",
    "                    page = tif.pages[0]\n",
    "                    if len(page.shape) >= 2:\n",
    "                        result[\"SizeX\"] = page.shape[-1]\n",
    "                        result[\"SizeY\"] = page.shape[-2]\n",
    "\n",
    "                # Get physical sizes from OME XML\n",
    "                ome_xml = tif.ome_metadata\n",
    "                if ome_xml:\n",
    "                    root = ET.fromstring(ome_xml)\n",
    "                    # Handle OME namespace\n",
    "                    ns = \"\"\n",
    "                    if \"}\" in root.tag:\n",
    "                        ns = root.tag.split(\"}\")[0] + \"}\"\n",
    "                    pixels = root.find(f\".//{ns}Pixels\")\n",
    "                    if pixels is not None:\n",
    "                        result[\"SizeX\"] = int(pixels.get(\"SizeX\", result.get(\"SizeX\", 0)))\n",
    "                        result[\"SizeY\"] = int(pixels.get(\"SizeY\", result.get(\"SizeY\", 0)))\n",
    "                        phys_x = pixels.get(\"PhysicalSizeX\")\n",
    "                        phys_y = pixels.get(\"PhysicalSizeY\")\n",
    "                        if phys_x is not None:\n",
    "                            result[\"PhysicalSizeX\"] = float(phys_x)\n",
    "                        if phys_y is not None:\n",
    "                            result[\"PhysicalSizeY\"] = float(phys_y)\n",
    "                        unit_x = pixels.get(\"PhysicalSizeXUnit\", \"µm\")\n",
    "                        unit_y = pixels.get(\"PhysicalSizeYUnit\", \"µm\")\n",
    "                        result[\"PhysicalSizeXUnit\"] = unit_x\n",
    "                        result[\"PhysicalSizeYUnit\"] = unit_y\n",
    "                        result[\"PhysicalSizeUnitX\"] = unit_x\n",
    "                        result[\"PhysicalSizeUnitY\"] = unit_y\n",
    "\n",
    "                return result if result else None\n",
    "    except Exception as e:\n",
    "        print(f\"    Could not read TIFF header from {image_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_image_metadata(image_url):\n",
    "    \"\"\"Fetch OME-TIFF metadata, trying metadata.json first, then TIFF header.\n",
    "\n",
    "    metadata.json may not contain SizeX/SizeY (only PhysicalSize fields),\n",
    "    so we merge with TIFF header data if pixel dimensions are missing.\n",
    "    \"\"\"\n",
    "    # Try metadata.json sidecar first (faster, smaller download)\n",
    "    meta_url = metadata_url_from_image_url(image_url)\n",
    "    json_meta = fetch_metadata_json(meta_url)\n",
    "\n",
    "    if json_meta is not None and json_meta.get(\"SizeX\") and json_meta.get(\"SizeY\"):\n",
    "        # metadata.json has everything we need\n",
    "        return \"metadata.json\", json_meta\n",
    "\n",
    "    # Need TIFF header — either metadata.json is missing entirely,\n",
    "    # or it lacks pixel dimensions (SizeX/SizeY)\n",
    "    tiff_meta = fetch_ome_metadata_from_tiff(image_url)\n",
    "\n",
    "    if json_meta is not None and tiff_meta is not None:\n",
    "        # Merge: use metadata.json as base, fill in missing fields from TIFF\n",
    "        merged = {**tiff_meta, **json_meta}\n",
    "        # Ensure SizeX/SizeY come from TIFF if not in json\n",
    "        if not json_meta.get(\"SizeX\") and tiff_meta.get(\"SizeX\"):\n",
    "            merged[\"SizeX\"] = tiff_meta[\"SizeX\"]\n",
    "        if not json_meta.get(\"SizeY\") and tiff_meta.get(\"SizeY\"):\n",
    "            merged[\"SizeY\"] = tiff_meta[\"SizeY\"]\n",
    "        return \"metadata.json+tiff_header\", merged\n",
    "\n",
    "    if json_meta is not None:\n",
    "        return \"metadata.json\", json_meta\n",
    "\n",
    "    if tiff_meta is not None:\n",
    "        return \"tiff_header\", tiff_meta\n",
    "\n",
    "    return \"none\", None\n",
    "\n",
    "\n",
    "for r in results:\n",
    "    if r[\"status\"] != \"success\":\n",
    "        continue\n",
    "\n",
    "    base_source, base_meta = fetch_image_metadata(r.get(\"base_image_url\"))\n",
    "    seg_source, seg_meta = fetch_image_metadata(r.get(\"seg_image_url\"))\n",
    "\n",
    "    r[\"base_metadata\"] = base_meta\n",
    "    r[\"seg_metadata\"] = seg_meta\n",
    "    r[\"base_meta_source\"] = base_source\n",
    "    r[\"seg_meta_source\"] = seg_source\n",
    "\n",
    "    r[\"base_pixel_x\"] = base_meta.get(\"SizeX\") if base_meta else None\n",
    "    r[\"base_pixel_y\"] = base_meta.get(\"SizeY\") if base_meta else None\n",
    "    r[\"seg_pixel_x\"] = seg_meta.get(\"SizeX\") if seg_meta else None\n",
    "    r[\"seg_pixel_y\"] = seg_meta.get(\"SizeY\") if seg_meta else None\n",
    "\n",
    "    print(\n",
    "        f\"  {r['hubmap_id']}: \"\n",
    "        f\"base={base_source} ({r.get('base_pixel_x')}x{r.get('base_pixel_y')}), \"\n",
    "        f\"seg={seg_source} ({r.get('seg_pixel_x')}x{r.get('seg_pixel_y')})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_physical_dimensions(metadata, pixel_x, pixel_y):\n",
    "    \"\"\"Compute total physical dimensions from pixel size and physical size per pixel.\"\"\"\n",
    "    if metadata is None or pixel_x is None or pixel_y is None:\n",
    "        return None, None, None\n",
    "\n",
    "    phys_x = metadata.get(\"PhysicalSizeX\")\n",
    "    phys_y = metadata.get(\"PhysicalSizeY\")\n",
    "    unit_x = metadata.get(\"PhysicalSizeUnitX\", metadata.get(\"PhysicalSizeXUnit\", \"\"))\n",
    "    unit_y = metadata.get(\"PhysicalSizeUnitY\", metadata.get(\"PhysicalSizeYUnit\", \"\"))\n",
    "\n",
    "    if phys_x is None or phys_y is None:\n",
    "        return None, None, None\n",
    "\n",
    "    total_x = pixel_x * phys_x\n",
    "    total_y = pixel_y * phys_y\n",
    "    unit = unit_x or unit_y\n",
    "\n",
    "    return total_x, total_y, unit\n",
    "\n",
    "\n",
    "summary_rows = []\n",
    "for r in results:\n",
    "    # Compute physical dimensions\n",
    "    base_total_x, base_total_y, base_unit = compute_physical_dimensions(\n",
    "        r.get(\"base_metadata\"), r.get(\"base_pixel_x\"), r.get(\"base_pixel_y\")\n",
    "    )\n",
    "    seg_total_x, seg_total_y, seg_unit = compute_physical_dimensions(\n",
    "        r.get(\"seg_metadata\"), r.get(\"seg_pixel_x\"), r.get(\"seg_pixel_y\")\n",
    "    )\n",
    "\n",
    "    row = {\n",
    "        \"HuBMAP ID\": r.get(\"hubmap_id\", \"\"),\n",
    "        \"UUID\": r.get(\"uuid\", \"\"),\n",
    "        \"Parent UUID\": r.get(\"parent_uuid\", \"\"),\n",
    "        \"Vis-Lifted\": r.get(\"vis_lifted_uuid\", \"\") or \"\",\n",
    "        \"Status\": r.get(\"status\"),\n",
    "        \"Error\": r.get(\"error\", \"\"),\n",
    "        \"Base Image Source\": r.get(\"base_image_source\", \"\"),\n",
    "        \"Metadata Source\": f\"base={r.get('base_meta_source', 'N/A')}, seg={r.get('seg_meta_source', 'N/A')}\",\n",
    "        \"Base Pixel (W x H)\": (\n",
    "            f\"{r.get('base_pixel_x')} x {r.get('base_pixel_y')}\"\n",
    "            if r.get(\"base_pixel_x\") is not None\n",
    "            else \"N/A\"\n",
    "        ),\n",
    "        \"Seg Pixel (W x H)\": (\n",
    "            f\"{r.get('seg_pixel_x')} x {r.get('seg_pixel_y')}\"\n",
    "            if r.get(\"seg_pixel_x\") is not None\n",
    "            else \"N/A\"\n",
    "        ),\n",
    "        \"Base Physical (W x H)\": (\n",
    "            f\"{base_total_x:.2f} x {base_total_y:.2f} {base_unit}\"\n",
    "            if base_total_x is not None\n",
    "            else \"N/A\"\n",
    "        ),\n",
    "        \"Seg Physical (W x H)\": (\n",
    "            f\"{seg_total_x:.2f} x {seg_total_y:.2f} {seg_unit}\"\n",
    "            if seg_total_x is not None\n",
    "            else \"N/A\"\n",
    "        ),\n",
    "    }\n",
    "    summary_rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(summary_rows)\n",
    "print(f\"Total datasets: {len(df)}\")\n",
    "print(f\"  Successful: {len(df[df['Status'] == 'success'])}\")\n",
    "print(f\"  Failed: {len(df[df['Status'] == 'error'])}\")\n",
    "print(f\"  Vis-lifted: {len(df[df['Vis-Lifted'] != ''])}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to CSV for external analysis\n",
    "df.to_csv(\"kaggle1_segmentation_alignment_summary.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hubmap-portal-ui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
